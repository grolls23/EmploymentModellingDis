





# Basics
import pandas as pd
import geopandas as gpd
import numpy as np

# Plots and Stats
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import textwrap

# Spatial Autocorrelation
from libpysal.weights import Queen
from esda.moran import Moran

# Geometry
from shapely import wkt
from shapely.wkt import loads
from shapely.geometry import Polygon, MultiPolygon





# London & Birmingham All Data

all_data_london = pd.read_csv("data/combined_data/lag/all_data_london_lag.csv")
all_data_bham = pd.read_csv("data/combined_data/lag/all_data_bham_lag.csv")


# LSOA Atlas Features

lsoa_atlas = pd.read_csv("data/lsoa_data/lsoa_atlas_preprocessed.csv", header = None)

#Condense Header into One
header_row1 = lsoa_atlas.iloc[0].astype(str)
header_row2 = lsoa_atlas.iloc[1].astype(str)

new_header = header_row1 + ' ' + header_row2
lsoa_atlas.columns = new_header
lsoa_atlas = lsoa_atlas[2:]

lsoa_atlas.rename(columns={'nan Codes': 'LSOA11CD', 'nan Names': 'LSOA11NM'}, inplace=True)

census_year_columns = [col for col in lsoa_atlas.columns if col.endswith('2011')]

# Create a new DataFrame with only the last year columns
lsoa_atlas_last_year = lsoa_atlas[['LSOA11CD', 'LSOA11NM'] + census_year_columns]

# Reset index
lsoa_atlas.reset_index(drop=True, inplace=True)

lsoa_atlas.head()


# POI Modelling Results (Models 25 + 26)

# log Employment
london_model = pd.read_csv("data/combined_data/model_results_london_poi.csv", index_col=0)
bham_model = pd.read_csv("data/combined_data/model_results_bham_poi.csv", index_col=0)

# Employment Density
# london_model = pd.read_csv("data/combined_data/model_results_london_poi_density.csv", index_col=0)
# bham_model = pd.read_csv("data/combined_data/model_results_bham_poi_density.csv", index_col=0)


# Convert Geometry col to make GeoDataFrame
london_model['geometry'] = london_model['geometry'].apply(loads)
london_model = gpd.GeoDataFrame(london_model, crs='EPSG:4326', geometry = london_model['geometry'])

bham_model['geometry'] = bham_model['geometry'].apply(loads)
bham_model = gpd.GeoDataFrame(bham_model, crs='EPSG:4326', geometry = bham_model['geometry'])

london_model.head()


# Select Outliers - Z score over or under 3

# Create residual column (predicted - actual)
london_model['residual'] = london_model['observed'] - london_model['predicted']
bham_model['residual'] = bham_model['observed'] - bham_model['predicted']

bham_model.head()



# Plot Residuals London (log)

vmax = max(abs(london_model['residual'].max()), abs(london_model['residual'].min()))

fig, ax = plt.subplots(1, 1, figsize=(20, 15))
london_model.plot(ax=ax, column='residual', linewidth=0, legend = True, cmap='seismic',
                      vmin=-vmax, vmax=vmax,
                     legend_kwds = {
                         'label': "Residual (observed - predicted)",
                        'orientation': "horizontal",
                        'pad' : 0.00,
                        'shrink': 0.4,
                        'aspect': 30})

ax.set_axis_off()
plt.show()


# Plot Residuals Birmingham (log)

vmax = max(abs(bham_model['residual'].max()), abs(bham_model['residual'].min()))

fig, ax = plt.subplots(1, 1, figsize=(20, 15))
bham_model.plot(ax=ax, column='residual', linewidth=0, legend = True, cmap='seismic',
                      vmin=-vmax, vmax=vmax,
                     legend_kwds = {
                         'label': "Residual (observed - predicted)",
                        'orientation': "horizontal",
                        'pad' : 0.00,
                        'shrink': 0.4,
                        'aspect': 30})

ax.set_axis_off()
plt.show()


# Underpredicted London

# Underpredicted threshold

# (log) Employment
under_condition = (london_model['residual'] > 1.5)

# Employment density
# condition = (london_model['residual'] > 0.04)

london_model['underpredicted'] = np.where(under_condition, london_model['residual'], 0)

vmax = max(abs(london_model['underpredicted'].max()), abs(london_model['underpredicted'].min()))

cmap_outliers = plt.cm.get_cmap('seismic')

fig, ax = plt.subplots(1, 1, figsize=(20, 15))
london_model.plot(ax=ax, column='underpredicted', linewidth=0.1, edgecolor='black', legend = False, cmap=cmap_outliers, vmin=-vmax, vmax=vmax)

ax.set_axis_off()
plt.show()


# Overpredicted London

# Overpredicted threshold

# (log) Employment
over_condition = (london_model['residual'] < -1.5)

# Employment density
# condition = (london_model['residual'] < -0.04)


london_model['overpredicted'] = np.where(over_condition, london_model['residual'], 0)

vmax = max(abs(london_model['underpredicted'].max()), abs(london_model['underpredicted'].min()))

fig, ax = plt.subplots(1, 1, figsize=(20, 15))
london_model.plot(ax=ax, column='overpredicted', linewidth=0.1, edgecolor='black', legend = False, cmap=cmap_outliers, vmin=-vmax, vmax=vmax)

ax.set_axis_off()
plt.show()


# Combined plot

london_model['outliers'] = np.where(under_condition, london_model['residual'], 
                                    np.where(over_condition, london_model['residual'], 0))

# Set the maximum value for the color scale
vmax = max(abs(london_model['outliers'].max()), abs(london_model['outliers'].min()))

# Use a divergent colormap to differentiate between underpredicted and overpredicted
cmap_outliers = plt.cm.get_cmap('seismic')

# Plot the combined outliers
fig, ax = plt.subplots(1, 1, figsize=(20, 15))
london_model.plot(ax=ax, column='outliers', linewidth=0.1, edgecolor='black', legend=False, cmap=cmap_outliers, vmin=-vmax, vmax=vmax)

# Remove the axis for a cleaner look
ax.set_axis_off()
plt.show()


# Underpredicted Birmingham

# Underpredicted threshold

# (log) Employment
under_condition = (bham_model['residual'] > 1.5)

# Employment Density
# condition = (bham_model['residual'] > 0.04)


bham_model['underpredicted'] = np.where(under_condition, bham_model['residual'], 0)

vmax = max(abs(bham_model['underpredicted'].max()), abs(bham_model['underpredicted'].min()))

fig, ax = plt.subplots(1, 1, figsize=(20, 15))
bham_model.plot(ax=ax, column='underpredicted', linewidth=0.1, edgecolor='black', legend = False, cmap=cmap_outliers, vmin=-vmax, vmax=vmax)


ax.set_axis_off()
plt.show()


# Overpredicted Birmingham

# Overpredicted threshold

# (log) Employment
over_condition = (bham_model['residual'] < -1.5)

# Employment density
# over_condition = (bham_model['residual'] < -0.04)


bham_model['overpredicted'] = np.where(over_condition, bham_model['residual'], 0)

vmax = max(abs(bham_model['overpredicted'].max()), abs(bham_model['overpredicted'].min()))

fig, ax = plt.subplots(1, 1, figsize=(20, 15))
bham_model.plot(ax=ax, column='overpredicted', linewidth=0.1, edgecolor='black', legend = False, cmap=cmap_outliers, vmin=-vmax, vmax=vmax)

ax.set_axis_off()
plt.show()


# Combined plot

bham_model['outliers'] = np.where(under_condition, bham_model['residual'], 
                                    np.where(over_condition, bham_model['residual'], 0))


vmax = max(abs(bham_model['outliers'].max()), abs(bham_model['outliers'].min()))

cmap_outliers = plt.cm.get_cmap('seismic')

fig, ax = plt.subplots(1, 1, figsize=(20, 15))
bham_model.plot(ax=ax, column='outliers', linewidth=0.1, edgecolor='black', legend=False, cmap=cmap_outliers, vmin=-vmax, vmax=vmax)

ax.set_axis_off()
plt.show()





# Feature Analysis of Outliers

# Create new dataframes with over and underpredicted LSOAS only
london_underpredicted = london_model[london_model['underpredicted'] != 0].copy()
london_overpredicted = london_model[london_model['overpredicted'] != 0].copy()

bham_underpredicted = bham_model[bham_model['underpredicted'] != 0].copy()
bham_overpredicted = bham_model[bham_model['overpredicted'] != 0].copy()

# Convert LSOA atlas data to numeric
columns_to_convert = lsoa_atlas.columns[2:]
lsoa_atlas[columns_to_convert] = lsoa_atlas[columns_to_convert].apply(pd.to_numeric, errors="coerce")

# Join all London on LSOA atlas

london_underpredicted_stats = pd.merge(lsoa_atlas, london_underpredicted[['name']], left_on='LSOA11CD', right_on='name', how='inner')
london_overpredicted_stats = pd.merge(lsoa_atlas, london_overpredicted[['name']], left_on='LSOA11CD', right_on='name', how='inner')

# And remainders (for T-test)
london_underpredicted_remainder = lsoa_atlas[~lsoa_atlas['LSOA11CD'].isin(london_underpredicted_stats['LSOA11CD'])]
london_overpredicted_remainder = lsoa_atlas[~lsoa_atlas['LSOA11CD'].isin(london_overpredicted_stats['LSOA11CD'])]

london_underpredicted_stats.head()


# T-Test Analysis on LSOA Categories

# Get features (all columns in preprocessed atlas)
features = lsoa_atlas.columns[2:-1]

# T-Test calculations

results = {}
t_results = []

for feature in features:
    underpredicted_mean = london_underpredicted_stats[feature].mean()
    underpredicted_remainder_mean = london_underpredicted_remainder[feature].mean()
    overpredicted_mean = london_overpredicted_stats[feature].mean()
    overpredicted_remainder_mean = london_overpredicted_remainder[feature].mean()
    
    stat_underpredicted, pval_underpredicted = stats.ttest_ind(london_underpredicted_stats[feature], london_underpredicted_remainder[feature])
    stat_overpredicted, pval_overpredicted = stats.ttest_ind(london_overpredicted_stats[feature], london_overpredicted_remainder[feature])
    
    results[feature] = {
        'Underpredicted': {'t-statistic': stat_underpredicted, 'p-value': pval_underpredicted},
        'Overpredicted': {'t-statistic': stat_overpredicted, 'p-value': pval_overpredicted}
    }

    t_results.append({
        'Feature': feature,
        'Underpredicted_t-statistic': stat_underpredicted,
        'Underpredicted_p-value': pval_underpredicted,
        'Underpredicted_mean': underpredicted_mean,
        'Underpredicted_Remainder_mean': underpredicted_remainder_mean,
        'Overpredicted_t-statistic': stat_overpredicted,
        'Overpredicted_p-value': pval_overpredicted,
        'Overpredicted_mean': overpredicted_mean,
        'Overpredicted_Remainder_mean': overpredicted_remainder_mean
    })



# Process results dataframe
t_results = pd.DataFrame(t_results)

# Sort (under & overpredicted)
t_results_underpredicted = t_results.sort_values(by='Underpredicted_p-value')
t_results_overpredicted = t_results.sort_values(by='Overpredicted_p-value')


t_results_underpredicted[['Feature', 'Underpredicted_t-statistic', 'Underpredicted_p-value', 'Underpredicted_mean', 'Underpredicted_Remainder_mean']].head(25)



t_results_overpredicted[['Feature', 'Overpredicted_t-statistic', 'Overpredicted_p-value', 'Overpredicted_mean', 'Overpredicted_Remainder_mean']].head(25)






# Spatial Autocorrelation Analysis of (all) residuals - London

# Check for missing residuals
if london_model['residual'].isnull().sum() > 0:
    print("Dropping missing")
    london_model = london_model.dropna(subset=['residual'])

# Weights Matrix (Queen's Case)
weights = Queen.from_dataframe(london_model)

# Moran's I
moran = Moran(london_model['residual'], weights)

print(f"Moran's I: {moran.I}")
print(f"Expected I: {moran.EI}")
print(f"Variance of I: {moran.VI_norm}")
print(f"Z-score: {moran.z_norm}")
print(f"P-value: {moran.p_norm}")


# Spatial Autocorrelation Analysis of (all) residuals - Birmingham

# Check for missing residuals
if bham_model['residual'].isnull().sum() > 0:
    print("Dropping missing")
    bham_model = bham_model.dropna(subset=['residual'])

# Weights Matrix (Queen's Case)
weights = Queen.from_dataframe(bham_model)

# Moran's I
moran = Moran(bham_model['residual'], weights)

print(f"Moran's I: {moran.I}")
print(f"Expected I: {moran.EI}")
print(f"Variance of I: {moran.VI_norm}")
print(f"Z-score: {moran.z_norm}")
print(f"P-value: {moran.p_norm}")





# Join residual data with features

correlation_data = pd.merge(lsoa_atlas, london_model, left_on='LSOA11CD', right_on='name', how='inner')

correlation_data.head()


# Correlation Matrix

# Get columns
lsoa_atlas_columns = list(lsoa_atlas.columns)
selected_columns = lsoa_atlas_columns[2:]

correlations = {}
for col in selected_columns:
    if col in correlation_data.columns:
        correlation = correlation_data['residual'].corr(correlation_data[col])
        correlations[col] = correlation

# Sort descending
correlation_df = pd.DataFrame(list(correlations.items()), columns=['Column', 'Correlation'])
correlation_df = correlation_df.sort_values(by='Correlation', ascending=False)

# Make barplot
plt.figure(figsize=(20, 16))
sns.barplot(x='Correlation', y='Column', data=correlation_df, palette='viridis')
plt.title('Correlation between residual and selected demographic measures')
plt.xlabel('Correlation with Residual')
plt.ylabel('Field')
plt.xticks(rotation=90)
plt.grid(True, linestyle='--', alpha=0.7)

plt.show()


plt.figure(figsize=(12, len(selected_columns) * 4))

# Plot each correlation
for i, col in enumerate(selected_columns):
    if col in correlation_data.columns:
        plt.subplot(len(selected_columns), 1, i + 1)  # Create subplot for each selected column
        sns.scatterplot(x=correlation_data['residual'], y=correlation_data[col])
        plt.title(f'Correlation between residual and {col}')
        plt.xlabel('residual')
        plt.ylabel(col)
        plt.grid(True)

# Adjust layout to prevent overlap
plt.tight_layout()
plt.show()





# Same Analysis on Education, Employment Status, and Multiple Deprivation Data

general_health = pd.read_csv("data/lsoa_data/TS037_general_health.csv", skiprows = 7, header = 0)
employment_residential = pd.read_csv("data/lsoa_data/TS066_economic_activity_status.csv", skiprows = 7, header = 0)
education = pd.read_csv("data/lsoa_data/TS067_highest_qualification.csv", skiprows = 7, header = 0)
household_comp = pd.read_csv("data/lsoa_data/TS003_household_composition.csv", skiprows = 6, header = 0)
age_bands = pd.read_csv("data/lsoa_data/TS007B_age_broad_band.csv", skiprows = 4, header = 0)
english_lang = pd.read_csv("data/lsoa_data/TS029_english_language.csv", skiprows = 6, header = 0)

#Separate name into LSOA11CD and LSOA11NM (taken from DataCleaning.ipynb)
def split_column(value):
    if isinstance(value, str):
        code, name = value.split(' : ')
        return code.strip(), name.strip()
    else:
        return None, None

# Parse Code and Name out
general_health[['LSOA11CD', 'LSOA11NM']] = general_health['2021 super output area - lower layer'].apply(lambda x: pd.Series(split_column(x)))
employment_residential[['LSOA11CD', 'LSOA11NM']] = employment_residential['2021 super output area - lower layer'].apply(lambda x: pd.Series(split_column(x)))
education[['LSOA11CD', 'LSOA11NM']] = education['2021 super output area - lower layer'].apply(lambda x: pd.Series(split_column(x)))
household_comp[['LSOA11CD', 'LSOA11NM']] = household_comp['2021 super output area - lower layer'].apply(lambda x: pd.Series(split_column(x)))
age_bands[['LSOA11CD', 'LSOA11NM']] = age_bands['2021 super output area - lower layer'].apply(lambda x: pd.Series(split_column(x)))
english_lang[['LSOA11CD', 'LSOA11NM']] = english_lang['2021 super output area - lower layer'].apply(lambda x: pd.Series(split_column(x)))

# Drop original column
general_health = general_health.drop(columns=['2021 super output area - lower layer'])
employment_residential = employment_residential.drop(columns=['2021 super output area - lower layer'])
education = education.drop(columns=['2021 super output area - lower layer'])
household_comp = household_comp.drop(columns=['2021 super output area - lower layer'])
age_bands = age_bands.drop(columns=['2021 super output area - lower layer'])
english_lang = english_lang.drop(columns=['2021 super output area - lower layer'])


multiple_deprivation = pd.read_csv("data/lsoa_data/multiple_deprivation.csv", header = 0)
multiple_deprivation.rename(columns = {'LSOA code (2011)':'LSOA11CD', 'LSOA name (2011)':'LSOA11NM'}, inplace=True)
multiple_deprivation = multiple_deprivation.drop(columns=["Local Authority District code (2019)", "Local Authority District name (2019)"])


multiple_deprivation.head()



# Join all data together

combined_census = pd.merge(general_health, employment_residential, on='LSOA11CD', suffixes=('', '_drop'))
combined_census = pd.merge(combined_census, education, on='LSOA11CD', suffixes=('', '_drop'))
combined_census = pd.merge(combined_census, household_comp, on='LSOA11CD', suffixes=('', '_drop'))
combined_census = pd.merge(combined_census, age_bands, on='LSOA11CD', suffixes=('', '_drop'))
combined_census = pd.merge(combined_census, english_lang, on='LSOA11CD', suffixes=('', '_drop'))
combined_census = pd.merge(combined_census, multiple_deprivation, on='LSOA11CD', suffixes=('', '_drop'))

# Drop duplicates
combined_census = combined_census[[col for col in combined_census.columns if not col.endswith('_drop')]]

# Join with london and birmingham model output data
combined_model = pd.concat([london_model, bham_model])
combined_census = combined_census.merge(combined_model[['name','residual']], left_on='LSOA11CD', right_on='name', how='inner').drop(columns=['name'])

# Fix string rank data
combined_census['Index of Multiple Deprivation (IMD) Rank'] = combined_census['Index of Multiple Deprivation (IMD) Rank'].str.replace(',', '')
combined_census['Index of Multiple Deprivation (IMD) Rank']  = pd.to_numeric(combined_census['Index of Multiple Deprivation (IMD) Rank'] )   

combined_census.head()


# More Correlation Analysis - Some code taken from above

# Get columns
combined_census_columns = list(combined_census.columns)
exclude_columns = ['LSOA11CD', 'LSOA11NM', 'residual', 'Total: All usual residents', 'Total: All usual residents aged 16 years and over']
feature_columns = [col for col in combined_census.columns if col not in exclude_columns]

correlations = {}
for col in feature_columns:
    if col in combined_census.columns:
        correlation = combined_census['residual'].corr(combined_census[col])
        correlations[col] = correlation

# Sort descending
correlation_df = pd.DataFrame(list(correlations.items()), columns=['Column', 'Correlation'])
correlation_df = correlation_df.sort_values(by='Correlation', ascending=False)

# Wrap label text function
def wrap_labels(ax, width, labels):
    wrapped_labels = [textwrap.fill(label, width) for label in labels]
    ax.set_yticklabels(wrapped_labels, fontsize=18)

# Make barplot
plt.figure(figsize=(20, 30))
ax_all = sns.barplot(x='Correlation', y='Column', data=correlation_df, palette='viridis')
# plt.title('Correlation between residual and selected demographic measures')
plt.xlabel('Correlation with Residual')
plt.ylabel('Field')
plt.xticks(rotation=90, fontsize = 20)
plt.yticks(fontsize=18)
#wrap_labels(ax_all, 60, correlation_df['Column']) 
plt.grid(True, linestyle='--', alpha=0.7)

path = "all"
# plt.savefig(path, bbox_inches='tight')
plt.show()

plt.show()


# Version two - only correlations over 0.05

# Get columns
combined_census_columns = list(combined_census.columns)
exclude_columns = ['LSOA11CD', 'LSOA11NM', 'residual', 'Total: All usual residents', 'Total: All usual residents aged 16 years and over']
feature_columns = [col for col in combined_census.columns if col not in exclude_columns]

correlations = {}
for col in feature_columns:
    if col in combined_census.columns:
        correlation = combined_census['residual'].corr(combined_census[col])
        correlations[col] = correlation

correlation_df = pd.DataFrame(list(correlations.items()), columns=['Column', 'Correlation'])
correlation_df = correlation_df.sort_values(by='Correlation', ascending=False)

# Filter
positive_correlation_df = correlation_df[correlation_df['Correlation'] > 0.05]
negative_correlation_df = correlation_df[correlation_df['Correlation'] < -0.05]

# Wrap label text function
def wrap_labels(ax, width, labels):
    wrapped_labels = [textwrap.fill(label, width) for label in labels]
    ax.set_yticklabels(wrapped_labels, fontsize=24)

# Plot positive correlations
plt.figure(figsize=(20, 16))
ax1 = sns.barplot(x='Correlation', y='Column', data=positive_correlation_df, palette='viridis')
# plt.title('Positive Correlations (> 0.05) between Residual and Selected Demographic Measures')
plt.xlabel('Correlation with Residual', fontsize=18)
plt.ylabel('Field', fontsize=18)
plt.xticks(rotation=90, fontsize=18)
plt.yticks(fontsize=24)
wrap_labels(ax1, 50, positive_correlation_df['Column']) 
plt.grid(True, linestyle='--', alpha=0.7)

path = "positive"
# plt.savefig(path, bbox_inches='tight')
plt.show()

# Plot negative correlations
plt.figure(figsize=(20, 16))
ax2 = sns.barplot(x='Correlation', y='Column', data=negative_correlation_df, palette='viridis')
# plt.title('Negative Correlations (< -0.05) between Residual and Selected Demographic Measures')
plt.xlabel('Correlation with Residual', fontsize=18)
plt.ylabel('Field', fontsize=18)
plt.xticks(rotation=90, fontsize=18)
plt.yticks(fontsize=24)
wrap_labels(ax2, 50, negative_correlation_df['Column'])
plt.grid(True, linestyle='--', alpha=0.7)

path = "negative"
# plt.savefig(path, bbox_inches='tight')
plt.show()


# Scatterplots for each field

plt.figure(figsize=(12, len(feature_columns) * 4))

# Plot each correlation
for i, col in enumerate(feature_columns):
    if col in combined_census.columns:
        plt.subplot(len(feature_columns), 1, i + 1)  # Create subplot for each selected column
        sns.scatterplot(x=combined_census['residual'], y=combined_census[col])
        plt.title(f'Correlation between residual and {col}')
        plt.xlabel('Residual') # Or Rank or Decile for Multiple Deprivation
        plt.ylabel('Percentage')
        if col == 'Index of Multiple Deprivation (IMD) Rank':
            plt.ylabel('Rank')
        elif col == 'Index of Multiple Deprivation (IMD) Decile':
            plt.ylabel('Decile')
        else:
            plt.ylabel('Percentage')
        plt.grid(True)

# Adjust layout to prevent overlap
plt.tight_layout()
plt.show()


# Add features to london_underpredicted and london_overpredicted stats

# Create combined lists for both cities
combined_underpredicted = pd.concat([london_underpredicted, bham_underpredicted])
combined_overpredicted = pd.concat([london_overpredicted, bham_overpredicted])


combined_underpredicted_stats = pd.merge(combined_census, combined_underpredicted, left_on='LSOA11CD', right_on='name', how='inner')
combined_overpredicted_stats = pd.merge(combined_census, combined_overpredicted, left_on='LSOA11CD', right_on='name', how='inner')

# And remainders (for T-test)
combined_underpredicted_remainder = combined_census[~combined_census['LSOA11CD'].isin(combined_underpredicted_stats['LSOA11CD'])]
combined_overpredicted_remainder = combined_census[~combined_census['LSOA11CD'].isin(combined_overpredicted_stats['LSOA11CD'])]


# More T-tests on specific outliers here? Would be good because there are some outlier specific trends that become clear

# T-Test Analysis on LSOA Categories

# Get features (all columns in preprocessed atlas)
features = feature_columns

# T-Test calculations

results = {}
t_results = []

for feature in features:
    underpredicted_mean = combined_underpredicted_stats[feature].mean()
    underpredicted_remainder_mean = combined_underpredicted_remainder[feature].mean()
    overpredicted_mean = combined_overpredicted_stats[feature].mean()
    overpredicted_remainder_mean = combined_overpredicted_remainder[feature].mean()
    
    stat_underpredicted, pval_underpredicted = stats.ttest_ind(combined_underpredicted_stats[feature], combined_underpredicted_remainder[feature])
    stat_overpredicted, pval_overpredicted = stats.ttest_ind(combined_overpredicted_stats[feature], combined_overpredicted_remainder[feature])
    
    results[feature] = {
        'Underpredicted': {'t-statistic': stat_underpredicted, 'p-value': pval_underpredicted},
        'Overpredicted': {'t-statistic': stat_overpredicted, 'p-value': pval_overpredicted}
    }

    t_results.append({
        'Feature': feature,
        'Underpredicted_t-statistic': stat_underpredicted,
        'Underpredicted_p-value': pval_underpredicted,
        'Underpredicted_mean': underpredicted_mean,
        'Underpredicted_Remainder_mean': underpredicted_remainder_mean,
        'Overpredicted_t-statistic': stat_overpredicted,
        'Overpredicted_p-value': pval_overpredicted,
        'Overpredicted_mean': overpredicted_mean,
        'Overpredicted_Remainder_mean': overpredicted_remainder_mean
    })



# Process results dataframe - again taken from above
t_results = pd.DataFrame(t_results)

# Sort (under & overpredicted)
t_results_underpredicted = t_results.sort_values(by='Underpredicted_p-value')
t_results_overpredicted = t_results.sort_values(by='Overpredicted_p-value')


t_results_underpredicted[['Feature', 'Underpredicted_t-statistic', 'Underpredicted_p-value', 'Underpredicted_mean', 'Underpredicted_Remainder_mean']].head(25)



t_results_overpredicted[['Feature', 'Overpredicted_t-statistic', 'Overpredicted_p-value', 'Overpredicted_mean', 'Overpredicted_Remainder_mean']].head(65)






# Separate out analysis for London and Birmingham specifically

# London Census Data
london_census = combined_census.merge(london_model[['name','residual']], left_on='LSOA11CD', right_on='name', how='inner').drop(columns=['name'])

london_census.head()

# Fix string rank data
combined_census['Index of Multiple Deprivation (IMD) Rank'] = combined_census['Index of Multiple Deprivation (IMD) Rank'].str.replace(',', '')
combined_census['Index of Multiple Deprivation (IMD) Rank']  = pd.to_numeric(combined_census['Index of Multiple Deprivation (IMD) Rank'] )   

combined_census.head()
