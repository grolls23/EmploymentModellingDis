





#Basics
import pandas as pd
import geopandas as gpd
import numpy as np
import csv

#Shapely
from geopy.geocoders import Nominatim
from shapely.geometry import box
from shapely import wkt
import shapely.geometry
from shapely.geometry import Polygon, MultiPolygon
from shapely.geometry import mapping
from shapely.geometry import shape
from scipy.spatial import KDTree

#Plots
import matplotlib.pyplot as plt

# Machine Learning
from sklearn.preprocessing import OneHotEncoder
import ast

#Warning Supression
import warnings








#Skip the first six rows because they're header information
empl_data_london = pd.read_csv('data/employment_data/lsoa_by_industry_london.csv', skiprows=7, delimiter=',')

unnamed_cols_london = empl_data_london.columns[empl_data_london.columns.str.contains('^Unnamed:')]
empl_data_london.drop(columns=unnamed_cols_london, inplace=True)

#Separate name into LSOA11CD and LSOA11NM
def split_column(value):
    #Keep Greater London stats
    if value.startswith('gor:'):
        return value, value
        #Split into name and code
    else:
        parts = value.split('lsoa2011:')[1]
        code, name = parts.split(' : ')
        return code.strip(), name.strip()
        return code, name

empl_data_london[['LSOA11CD', 'LSOA11NM']] = empl_data_london['Area'].apply(lambda x: pd.Series(split_column(x)))

print("Num Rows (LSOAs) Before Cleaning: " + str(empl_data_london.shape[0]))

#There appear to be a bunch of duplicates so I'm going to get rid of them now
empl_data_london.drop_duplicates(inplace=True)

print("Num Rows (LSOAs) After Cleaning: " + str(empl_data_london.shape[0]))





# Get rid of value set on copy of slice warning
warnings.filterwarnings('ignore')

#Skip the first six rows because they're header information
empl_data_bham = pd.read_csv('data/employment_data/lsoa_by_industry.csv', skiprows=7, delimiter=',')

unnamed_cols_bham = empl_data_bham.columns[empl_data_bham.columns.str.contains('^Unnamed:')]
empl_data_bham.drop(columns=unnamed_cols_bham, inplace=True)

#Separate name into LSOA11CD and LSOA11NM
def split_column(value):
    if isinstance(value, str) and 'lsoa2011:' in value:
        parts = value.split('lsoa2011:')[1]
        code, name = parts.split(' : ')
        return code.strip(), name.strip()
    else:
        return None, None

empl_data_bham[['LSOA11CD', 'LSOA11NM']] = empl_data_bham['Area'].apply(lambda x: pd.Series(split_column(x)))

# Drop rows not in Birmingham
empl_data_bham.dropna(subset=['LSOA11NM'], inplace=True)
empl_data_bham = empl_data_bham[empl_data_bham['LSOA11NM'].str.contains('Birmingham')]

print("Num Rows (LSOAs) Before Cleaning: " + str(empl_data_bham.shape[0]))

#There appear to be a bunch of duplicates so I'm going to get rid of them now
empl_data_bham.drop_duplicates(inplace=True)

print("Num Rows (LSOAs) After Cleaning: " + str(empl_data_bham.shape[0]))


# Get rid of value set on copy of slice warning
warnings.filterwarnings('ignore')

# Create Total Employment and Log Total Employment Column for London
empl_data_london[empl_data_london.columns[1:-2]] = empl_data_london[empl_data_london.columns[1:-2]].apply(pd.to_numeric, errors='coerce')
empl_data_london['total_employment'] = empl_data_london[empl_data_london.columns[1:-2]].sum(axis=1)
empl_data_london['log_total_employment'] = np.log(empl_data_london['total_employment'].replace(0, np.nan))

empl_data_london.head()


# Get rid of value set on copy of slice warning
warnings.filterwarnings('ignore')

# Create Total Employment and Log Total Employment Column for Birmingham
empl_data_bham[empl_data_bham.columns[1:-2]] = empl_data_bham[empl_data_bham.columns[1:-2]].apply(pd.to_numeric, errors='coerce')
empl_data_bham['total_employment'] = empl_data_bham[empl_data_bham.columns[1:-2]].sum(axis=1)
empl_data_bham['log_total_employment'] = np.log(empl_data_bham['total_employment'].replace(0, np.nan))

empl_data_bham.head()





#Get LSOA Shapefile Data
lsoa_geo = gpd.read_file('data/lsoa_data/LSOA_2011_EW_BFE_V3.shp')

#Convert to WGS for consistency
lsoa_geo = lsoa_geo.to_crs(epsg=4326)

print("Num Rows (LSOAs): " + str(lsoa_geo.shape[0]))

#Get rid of columns I'm not using for now
lsoa_geo = lsoa_geo.drop(columns=['BNG_E', 'BNG_N', 'LONG_', 'LAT', 'GlobalID', 'Shape_Leng'])

#Join with population
lsoa_pop = pd.read_csv('data/lsoa_data/lsoa_pop.csv')

lsoa_geo.drop(columns=['LSOA11NM'])

#Get population estimates - had to be pulled in from separate Census dataset
lsoa_geo = lsoa_geo.merge(lsoa_pop, on='LSOA11CD')

lsoa_geo.head()








london_places = gpd.read_file('data/overture_data/london_places.geojson')

print('Number of POIs in Overture (London): ' + str(london_places.shape[0]))
london_places.head()





bham_places = gpd.read_file('data/overture_data/bham_places.geojson')

print('Number of POIs in Overture (Birmingham): ' + str(bham_places.shape[0]))
bham_places.head()








london_all_buildings = gpd.read_file('data/osm_data/all_london_buildings.geojson')

print('Number of Buildings in OSM (London): ' + str(london_all_buildings.shape[0]))
london_all_buildings.head()


# Commercial Buildings

london_commercial_buildings = gpd.read_file('data/osm_data/all_london_commercial_buildings.geojson')

print('Number of Commercial Buildings in OSM (London): ' + str(london_commercial_buildings.shape[0]))
london_commercial_buildings.head()


# Office Buildings

london_office_buildings = gpd.read_file('data/osm_data/all_london_office_buildings.geojson')

print('Number of Office Buildings in OSM (London): ' + str(london_office_buildings.shape[0]))
london_office_buildings.head()


# Residential Buildings

london_residential_buildings = gpd.read_file('data/osm_data/all_london_residential_buildings.geojson')

print('Number of Residential Buildings in OSM (London): ' + str(london_residential_buildings.shape[0]))
london_residential_buildings.head()


# Retail Buildings

london_retail_buildings = gpd.read_file('data/osm_data/all_london_retail_buildings.geojson')

print('Number of Retail Buildings in OSM (London): ' + str(london_retail_buildings.shape[0]))
london_retail_buildings.head()


# Assign a building type in the main DataFrame if the osmid is found in one of the other DataFrames

# I know that office and retail buildings could have commercial or office but office and retail will overwrite commercial
# (and are thus checked after) because they're more specific

london_all_buildings['building_type'] = 'none'

london_all_buildings.loc[london_all_buildings['osmid'].isin(london_commercial_buildings['osmid']), 'building_type'] = 'commercial'
london_all_buildings.loc[london_all_buildings['osmid'].isin(london_retail_buildings['osmid']), 'building_type'] = 'retail'
london_all_buildings.loc[london_all_buildings['osmid'].isin(london_office_buildings['osmid']), 'building_type'] = 'office'
london_all_buildings.loc[london_all_buildings['osmid'].isin(london_residential_buildings['osmid']), 'building_type'] = 'residential'

london_all_buildings.head(50)





bham_all_buildings = gpd.read_file('data/osm_data/bham_buildings.geojson')

print('Number of Buildings in OSM (Birmingham): ' + str(bham_all_buildings.shape[0]))
bham_all_buildings.head()


# Commercial Buildings

bham_commercial_buildings = gpd.read_file('data/osm_data/bham_commercial_buildings.geojson')

print('Number of Commercial Buildings in OSM (Birmingham): ' + str(bham_commercial_buildings.shape[0]))
bham_commercial_buildings.head()


# Office Buildings

bham_office_buildings = gpd.read_file('data/osm_data/bham_office_buildings.geojson')

print('Number of Office Buildings in OSM (Birmingham): ' + str(bham_office_buildings.shape[0]))
bham_office_buildings.head()


# Residential Buildings

bham_residential_buildings = gpd.read_file('data/osm_data/bham_residential_buildings.geojson')

print('Number of Residential Buildings in OSM (Birmingham): ' + str(bham_residential_buildings.shape[0]))
bham_residential_buildings.head()


# Retail Buildings

bham_retail_buildings = gpd.read_file('data/osm_data/bham_retail_buildings.geojson')

print('Number of Retail Buildings in OSM (Birmingham): ' + str(bham_retail_buildings.shape[0]))
bham_retail_buildings.head()


# Assign a building type in the main DataFrame if the osmid is found in one of the other DataFrames

# I know that office and retail buildings could have commercial or office but office and retail will overwrite commercial
# (and are thus checked after) because they're more specific

bham_all_buildings['building_type'] = 'none'

bham_all_buildings.loc[bham_all_buildings['osmid'].isin(bham_commercial_buildings['osmid']), 'building_type'] = 'commercial'
bham_all_buildings.loc[bham_all_buildings['osmid'].isin(bham_retail_buildings['osmid']), 'building_type'] = 'retail'
bham_all_buildings.loc[bham_all_buildings['osmid'].isin(bham_office_buildings['osmid']), 'building_type'] = 'office'
bham_all_buildings.loc[bham_all_buildings['osmid'].isin(bham_residential_buildings['osmid']), 'building_type'] = 'residential'

bham_all_buildings.head(50)











#Note that total London statistics are dropped here because there's no equivalent column in lsoa_geo
empl_geog_london = pd.merge(lsoa_geo, empl_data_london, on = "LSOA11CD")

print("Num Rows (LSOAS): " + str(empl_geog_london.shape[0]))

empl_geog_london.head()





#Note that total Birmingham statistics are dropped here because there's no equivalent column in lsoa_geo
empl_geog_bham = pd.merge(lsoa_geo, empl_data_bham, on = "LSOA11CD")

print("Num Rows (LSOAS): " + str(empl_geog_bham.shape[0]))

empl_geog_bham.head()





# London

# Dealing with filter warnings for empty geometries in particular LSOAs
warnings.filterwarnings("ignore", category=FutureWarning)

# Init Columns (This was causing an error at first when I didn't put it here to begin)
empl_geog_london['num_buildings'] = 0
empl_geog_london['num_retail_buildings'] = 0
empl_geog_london['num_residential_buildings'] = 0
empl_geog_london['num_commercial_buildings'] = 0
empl_geog_london['num_office_buildings'] = 0

empl_geog_london['num_places'] = 0

empl_geog_london['building_poly'] = None
empl_geog_london['commercial_building_poly'] = None
empl_geog_london['retail_building_poly'] = None
empl_geog_london['office_building_poly'] = None
empl_geog_london['residential_building_poly'] = None

empl_geog_london['place_points'] = None
empl_geog_london['category_list'] = None

# Iterate through all LSOAs and grab OSM building counts and subcategories - add to empl_geog_london dataframe
for index, row in empl_geog_london.iterrows():

    # Status report
    if (index % 20 == 0):
        print(index)
    
    geom = row['geometry']
    filter_geom = gpd.GeoSeries([geom], crs=lsoa_geo.crs)

    # All Buildings - OSM
    filtered_osm_buildings = london_all_buildings[london_all_buildings.geometry.intersects(filter_geom.unary_union)]

    # Get subcategories based on tags
    commercial_buildings = london_all_buildings[london_all_buildings['building_type'] == 'commercial']
    office_buildings = london_all_buildings[london_all_buildings['building_type'] == 'office']
    retail_buildings = london_all_buildings[london_all_buildings['building_type'] == 'retail']
    residential_buildings = london_all_buildings[london_all_buildings['building_type'] == 'residential']

    # Subcategories of each building
    filtered_commercial_buildings = commercial_buildings[commercial_buildings.geometry.intersects(filter_geom.unary_union)]
    filtered_office_buildings = office_buildings[office_buildings.geometry.intersects(filter_geom.unary_union)]
    filtered_retail_buildings = retail_buildings[retail_buildings.geometry.intersects(filter_geom.unary_union)]
    filtered_residential_buildings = residential_buildings[residential_buildings.geometry.intersects(filter_geom.unary_union)]

    # Multipolygon of each LSOA's (OSM) buildings - category separated
    combined_multipolygon = filtered_osm_buildings.geometry.unary_union
    combined_commercial_multipolygon = filtered_commercial_buildings.geometry.unary_union
    combined_office_polygon = filtered_office_buildings.geometry.unary_union
    combined_retail_polygon = filtered_retail_buildings.geometry.unary_union
    combined_residential_polygon = filtered_residential_buildings.geometry.unary_union
    
    # All Places
    filtered_places = london_places[london_places.geometry.intersects(filter_geom.unary_union)]

    # Multipoint of each LSOA's places
    combined_multipoint = filtered_places.geometry.unary_union
    
    # Add place category information
    category_list = filtered_places['category'].dropna().tolist()
    
    # Add back to Dataframe
    empl_geog_london.at[index, 'num_buildings'] = len(filtered_osm_buildings)
    empl_geog_london.at[index, 'log_num_buildings'] = np.log(len(filtered_osm_buildings) + 0.01)
    
    empl_geog_london.at[index, 'num_retail_buildings'] = len(filtered_retail_buildings)
    empl_geog_london.at[index, 'num_residential_buildings'] = len(filtered_residential_buildings)
    empl_geog_london.at[index, 'num_commercial_buildings'] = len(filtered_commercial_buildings)
    empl_geog_london.at[index, 'num_office_buildings'] = len(filtered_office_buildings)
    
    empl_geog_london.at[index, 'num_places'] = len(filtered_places)
    empl_geog_london.at[index, 'log_num_places'] = np.log(len(filtered_places) + 0.01)
    
    empl_geog_london.at[index, 'building_poly'] = combined_multipolygon
    empl_geog_london.at[index, 'commercial_building_poly'] = combined_commercial_multipolygon
    empl_geog_london.at[index, 'retail_building_poly'] = combined_office_polygon
    empl_geog_london.at[index, 'office_building_poly'] = combined_retail_polygon
    empl_geog_london.at[index, 'residential_building_poly'] = combined_residential_polygon

    empl_geog_london.at[index, 'place_points'] = combined_multipoint
    empl_geog_london.at[index, 'category_list'] = category_list
    
# Check
empl_geog_london.head()



# Birmingham

# Dealing with filter warnings for empty geometries in particular LSOAs
warnings.filterwarnings("ignore", category=FutureWarning)

# Init Columns (This was causing an error at first when I didn't put it here to begin)
empl_geog_bham['num_buildings'] = 0
empl_geog_bham['num_retail_buildings'] = 0
empl_geog_bham['num_residential_buildings'] = 0
empl_geog_bham['num_commercial_buildings'] = 0
empl_geog_bham['num_office_buildings'] = 0

empl_geog_bham['num_places'] = 0

empl_geog_bham['building_poly'] = None
empl_geog_bham['commercial_building_poly'] = None
empl_geog_bham['retail_building_poly'] = None
empl_geog_bham['office_building_poly'] = None
empl_geog_bham['residential_building_poly'] = None

empl_geog_bham['place_points'] = None
empl_geog_bham['category_list'] = None

# Iterate through all LSOAs and grab OSM building counts and subcategories - add to empl_geog_bham dataframe
for index, row in empl_geog_bham.iterrows():

    # Status report
    if (index % 20 == 0):
        print(index)
    
    geom = row['geometry']
    filter_geom = gpd.GeoSeries([geom], crs=lsoa_geo.crs)

    # All Buildings - OSM
    filtered_osm_buildings = bham_all_buildings[bham_all_buildings.geometry.intersects(filter_geom.unary_union)]

    # Get subcategories based on tags
    commercial_buildings = bham_all_buildings[bham_all_buildings['building_type'] == 'commercial']
    office_buildings = bham_all_buildings[bham_all_buildings['building_type'] == 'office']
    retail_buildings = bham_all_buildings[bham_all_buildings['building_type'] == 'retail']
    residential_buildings = bham_all_buildings[bham_all_buildings['building_type'] == 'residential']

    # Subcategories of each building
    filtered_commercial_buildings = commercial_buildings[commercial_buildings.geometry.intersects(filter_geom.unary_union)]
    filtered_office_buildings = office_buildings[office_buildings.geometry.intersects(filter_geom.unary_union)]
    filtered_retail_buildings = retail_buildings[retail_buildings.geometry.intersects(filter_geom.unary_union)]
    filtered_residential_buildings = residential_buildings[residential_buildings.geometry.intersects(filter_geom.unary_union)]

    # Multipolygon of each LSOA's (OSM) buildings - category separated
    combined_multipolygon = filtered_osm_buildings.geometry.unary_union
    combined_commercial_multipolygon = filtered_commercial_buildings.geometry.unary_union
    combined_office_polygon = filtered_office_buildings.geometry.unary_union
    combined_retail_polygon = filtered_retail_buildings.geometry.unary_union
    combined_residential_polygon = filtered_residential_buildings.geometry.unary_union
    
    # All Places
    filtered_places = bham_places[bham_places.geometry.intersects(filter_geom.unary_union)]

    # Multipoint of each LSOA's places
    combined_multipoint = filtered_places.geometry.unary_union
    
    # Add place category information
    category_list = filtered_places['category'].dropna().tolist()
    
    # Add back to Dataframe
    empl_geog_bham.at[index, 'num_buildings'] = len(filtered_osm_buildings)
    empl_geog_bham.at[index, 'log_num_buildings'] = np.log(len(filtered_osm_buildings) + 0.01)
    
    empl_geog_bham.at[index, 'num_retail_buildings'] = len(filtered_retail_buildings)
    empl_geog_bham.at[index, 'num_residential_buildings'] = len(filtered_residential_buildings)
    empl_geog_bham.at[index, 'num_commercial_buildings'] = len(filtered_commercial_buildings)
    empl_geog_bham.at[index, 'num_office_buildings'] = len(filtered_office_buildings)
    
    empl_geog_bham.at[index, 'num_places'] = len(filtered_places)
    empl_geog_bham.at[index, 'log_num_places'] = np.log(len(filtered_places) + 0.01)
        
    empl_geog_bham.at[index, 'building_poly'] = combined_multipolygon
    empl_geog_bham.at[index, 'commercial_building_poly'] = combined_commercial_multipolygon
    empl_geog_bham.at[index, 'retail_building_poly'] = combined_office_polygon
    empl_geog_bham.at[index, 'office_building_poly'] = combined_retail_polygon
    empl_geog_bham.at[index, 'residential_building_poly'] = combined_residential_polygon
    
    empl_geog_bham.at[index, 'place_points'] = combined_multipoint
    empl_geog_bham.at[index, 'category_list'] = category_list
    
# Check
empl_geog_bham.head()





# Save London to Dataframe on Desktop

# Convert multipolygons to WKT for export
empl_geog_london['building_poly'] = empl_geog_london['building_poly'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)
empl_geog_london['commercial_building_poly'] = empl_geog_london['commercial_building_poly'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)
empl_geog_london['retail_building_poly'] = empl_geog_london['retail_building_poly'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)
empl_geog_london['office_building_poly'] = empl_geog_london['office_building_poly'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)
empl_geog_london['residential_building_poly'] = empl_geog_london['residential_building_poly'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)

empl_geog_london['place_points'] = empl_geog_london['place_points'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)

# Convert category_list to string for export
empl_geog_london['category_list'] = empl_geog_london['category_list'].apply(lambda x: str(x) if x else None)

# Export
empl_geog_london.to_file("data/combined_data/empl_geog_london.geojson", driver="GeoJSON")



# Save Birmingham to Dataframe on Desktop

# Convert multipolygons to WKT for export
empl_geog_bham['building_poly'] = empl_geog_bham['building_poly'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)
empl_geog_bham['commercial_building_poly'] = empl_geog_bham['commercial_building_poly'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)
empl_geog_bham['retail_building_poly'] = empl_geog_bham['retail_building_poly'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)
empl_geog_bham['office_building_poly'] = empl_geog_bham['office_building_poly'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)
empl_geog_bham['residential_building_poly'] = empl_geog_bham['residential_building_poly'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)

empl_geog_bham['place_points'] = empl_geog_bham['place_points'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)

# Convert category_list to string for export
empl_geog_bham['category_list'] = empl_geog_bham['category_list'].apply(lambda x: str(x) if x else None)

# Export
empl_geog_bham.to_file("data/combined_data/empl_geog_bham.geojson", driver="GeoJSON")






# London
empl_geog_london = gpd.read_file("data/combined_data/empl_geog_london.geojson")

# WKT strings back to Shapely geometries
empl_geog_london['building_poly'] = empl_geog_london['building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
empl_geog_london['commercial_building_poly'] = empl_geog_london['commercial_building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
empl_geog_london['retail_building_poly'] = empl_geog_london['retail_building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
empl_geog_london['office_building_poly'] = empl_geog_london['office_building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
empl_geog_london['residential_building_poly'] = empl_geog_london['residential_building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)

empl_geog_london['place_points'] = empl_geog_london['place_points'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
# Convert string representations of lists back to lists
empl_geog_london['category_list'] = empl_geog_london['category_list'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)

# Birmingham
empl_geog_bham = gpd.read_file("data/combined_data/empl_geog_bham.geojson")

# WKT strings back to Shapely geometries
empl_geog_bham['building_poly'] = empl_geog_bham['building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
empl_geog_bham['commercial_building_poly'] = empl_geog_bham['commercial_building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
empl_geog_bham['retail_building_poly'] = empl_geog_bham['retail_building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
empl_geog_bham['office_building_poly'] = empl_geog_bham['office_building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
empl_geog_bham['residential_building_poly'] = empl_geog_bham['residential_building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)

empl_geog_bham['place_points'] = empl_geog_bham['place_points'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
# Convert string representations of lists back to lists
empl_geog_bham['category_list'] = empl_geog_bham['category_list'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)

# Test
empl_geog_london.head()








# Encode POI categories in data - using One-Hot encoding

#Make sure category list is a list
empl_geog_london['category_list'] = empl_geog_london['category_list'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)

#Explode category lists
empl_geog_london_exploded = empl_geog_london.explode('category_list')

#Set up encoder
encoder = OneHotEncoder(sparse_output=False)

#Encode
encoded_categories = encoder.fit_transform(empl_geog_london_exploded[['category_list']])
encoded_df = pd.DataFrame(encoded_categories, columns=encoder.get_feature_names_out(['category_list']))

empl_geog_london_exploded = pd.concat([empl_geog_london_exploded.reset_index(drop=True), encoded_df.reset_index(drop=True)], axis=1)

# Reaggregate
empl_geog_london_exploded.drop(columns=['category_list'], inplace=True)

# Group by LSOA11CD - sum numeric only
numeric_columns = encoded_df.columns.tolist()
empl_geog_london_encoded = empl_geog_london_exploded.groupby('LSOA11CD')[numeric_columns].sum().reset_index()

#Store category columns
category_columns = empl_geog_london_encoded.columns[1:]

empl_geog_london_encoded.head()





# Encode POI categories in data - using One-Hot encoding

#Make sure category list is a list
empl_geog_bham['category_list'] = empl_geog_bham['category_list'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)

#Explode category lists
empl_geog_bham_exploded = empl_geog_bham.explode('category_list')

#Set up encoder
encoder = OneHotEncoder(sparse_output=False)

#Encode
encoded_categories = encoder.fit_transform(empl_geog_bham_exploded[['category_list']])
encoded_df = pd.DataFrame(encoded_categories, columns=encoder.get_feature_names_out(['category_list']))

empl_geog_bham_exploded = pd.concat([empl_geog_bham_exploded.reset_index(drop=True), encoded_df.reset_index(drop=True)], axis=1)

# Reaggregate
empl_geog_bham_exploded.drop(columns=['category_list'], inplace=True)

# Group by LSOA11CD - sum numeric only
numeric_columns = encoded_df.columns.tolist()
empl_geog_bham_encoded = empl_geog_bham_exploded.groupby('LSOA11CD')[numeric_columns].sum().reset_index()

#Store category columns
category_columns = empl_geog_bham_encoded.columns[1:]

empl_geog_bham_encoded.head()





# Geometric Features Extraction (copied from earlier but includes building types now)

#I'm going to exclude num polygons, average perimeter, and total perimeter cause they're not so helpful (cause Collinearity matrix shows its very close to count)

def extract_multipolygon_features(multipolygon, lsoa_geometry):
    if isinstance(multipolygon, MultiPolygon):
        polygons = list(multipolygon.geoms)
    elif isinstance(multipolygon, Polygon):
        polygons = [multipolygon]
    else:
        return pd.Series({
            'total_area': 0,
            'avg_building_area': 0,
            'lsoa_area_ratio': 0,
        })

    num_polygons = len(polygons)
    areas = [polygon.area for polygon in polygons]
    total_area = sum(areas)
    avg_building_area = total_area / num_polygons if num_polygons > 0 else 0

    #Built-up area ratio could result in a div by zero error if there are no buildings in an LSOA so this logic has to be included here
    try:
        lsoa_area_ratio = total_area / lsoa_geometry.area
    except ZeroDivisionError:
        lsoa_area_ratio = 0

    return pd.Series({
        'total_area': total_area,
        'lsoa_area_ratio': lsoa_area_ratio,
        'avg_building_area': avg_building_area,
    })






# All Buildings
all_buildings_geometry_features = empl_geog_london.apply(
    lambda row: extract_multipolygon_features(row['building_poly'], row['geometry']),
    axis=1
).add_prefix('all_')

# Residential
residential_buildings_geometry_features = empl_geog_london.apply(
    lambda row: extract_multipolygon_features(row['residential_building_poly'], row['geometry']),
    axis=1
).add_prefix('residential_')

# Commercial
commercial_buildings_geometry_features = empl_geog_london.apply(
    lambda row: extract_multipolygon_features(row['commercial_building_poly'], row['geometry']),
    axis=1
).add_prefix('commercial_')

# Office
office_buildings_geometry_features = empl_geog_london.apply(
    lambda row: extract_multipolygon_features(row['office_building_poly'], row['geometry']),
    axis=1
).add_prefix('office_')

# Retail
retail_buildings_geometry_features = empl_geog_london.apply(
    lambda row: extract_multipolygon_features(row['retail_building_poly'], row['geometry']),
    axis=1
).add_prefix('retail_')

# Combine all geometry features into one DataFrame
all_geom_features_london = pd.concat([
    all_buildings_geometry_features,
    residential_buildings_geometry_features,
    commercial_buildings_geometry_features,
    office_buildings_geometry_features,
    retail_buildings_geometry_features
], axis=1, ignore_index=False)

# Capture Column Names
geo_features_london = all_geom_features_london.columns

# Display the first 50 rows
all_geom_features_london.head(50)





# All Buildings
all_buildings_geometry_features = empl_geog_bham.apply(
    lambda row: extract_multipolygon_features(row['building_poly'], row['geometry']),
    axis=1
).add_prefix('all_')

# Residential
residential_buildings_geometry_features = empl_geog_bham.apply(
    lambda row: extract_multipolygon_features(row['residential_building_poly'], row['geometry']),
    axis=1
).add_prefix('residential_')

# Commercial
commercial_buildings_geometry_features = empl_geog_bham.apply(
    lambda row: extract_multipolygon_features(row['commercial_building_poly'], row['geometry']),
    axis=1
).add_prefix('commercial_')

# Office
office_buildings_geometry_features = empl_geog_bham.apply(
    lambda row: extract_multipolygon_features(row['office_building_poly'], row['geometry']),
    axis=1
).add_prefix('office_')

# Retail
retail_buildings_geometry_features = empl_geog_bham.apply(
    lambda row: extract_multipolygon_features(row['retail_building_poly'], row['geometry']),
    axis=1
).add_prefix('retail_')

# Combine all geometry features into one DataFrame
all_geom_features_bham = pd.concat([
    all_buildings_geometry_features,
    residential_buildings_geometry_features,
    commercial_buildings_geometry_features,
    office_buildings_geometry_features,
    retail_buildings_geometry_features
], axis=1, ignore_index=False)

# Capture Column Names
geo_features_bham = all_geom_features_bham.columns

# Display the first 50 rows
all_geom_features_bham.head(50)


# Combine Datasets

# London

empl_geog_london['place_points'] = empl_geog_london['place_points'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)

data_with_geom_london = pd.concat([empl_geog_london, all_geom_features_london], axis=1)
all_data_london = pd.concat([data_with_geom_london, empl_geog_london_encoded], axis=1)

# Birmingham
empl_geog_bham['place_points'] = empl_geog_bham['place_points'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)

data_with_geom_bham = pd.concat([empl_geog_bham, all_geom_features_london], axis=1)
all_data_bham = pd.concat([data_with_geom_bham, empl_geog_bham_encoded], axis=1)






# Filter to quality places only

# London
quality_places_london = london_places[london_places['confidence'] >= 0.6]
quality_places_london.head()

# Birmingham
quality_places_bham = bham_places[bham_places['confidence'] >= 0.6]
quality_places_bham.head()


# London

# Dealing with filter warnings for empty geometries in particular LSOAs
warnings.filterwarnings("ignore", category=FutureWarning)

# Generating a version of empl_geog that has only quality places - I'm using my data_with_geom file as a starting point
empl_geog_quality_places_london = data_with_geom_london

#Copy back quality places

empl_geog_quality_places_london['category_list'] = None

for index, row in empl_geog_london.iterrows():
    geom = row['geometry']
    filter_geom = gpd.GeoSeries([geom], crs='EPSG:4326')

    #All Places
    filtered_places_london = quality_places_london[quality_places_london.geometry.intersects(filter_geom.unary_union)]

    #Multipoint of each LSOA's places
    combined_multipoint = filtered_places_london.geometry.unary_union
    
    #Add place category information
    category_list = filtered_places_london['category'].dropna().tolist()

    #Add back to Dataframe
    empl_geog_quality_places_london.at[index, 'category_list'] = category_list

empl_geog_quality_places_london.head()


# Birmingham

# Dealing with filter warnings for empty geometries in particular LSOAs
warnings.filterwarnings("ignore", category=FutureWarning)

# Generating a version of empl_geog that has only quality places - I'm using my data_with_geom file as a starting point
empl_geog_quality_places_bham = data_with_geom_bham

#Copy back quality places using logic from DataCleaning.ipynb

empl_geog_quality_places_bham['category_list'] = None

for index, row in empl_geog_bham.iterrows():
    geom = row['geometry']
    filter_geom = gpd.GeoSeries([geom], crs='EPSG:4326')

    #All Places
    filtered_places_bham = quality_places_bham[quality_places_bham.geometry.intersects(filter_geom.unary_union)]

    #Multipoint of each LSOA's places
    combined_multipoint = filtered_places_bham.geometry.unary_union
    
    #Add place category information
    category_list = filtered_places_bham['category'].dropna().tolist()

    #Add back to Dataframe
    empl_geog_quality_places_bham.at[index, 'category_list'] = category_list

empl_geog_quality_places_bham.head()


# One-Hot Encoding - yet again on London

#Explode category lists
empl_geog_quality_places_exploded_london = empl_geog_quality_places_london.explode('category_list')

#Set up encoder
encoder = OneHotEncoder(sparse_output=False)

#Encode
encoded_categories_quality = encoder.fit_transform(empl_geog_quality_places_exploded_london[['category_list']])
encoded_df = pd.DataFrame(encoded_categories_quality, columns=encoder.get_feature_names_out(['category_list']))

empl_geog_quality_places_exploded_london = pd.concat([empl_geog_quality_places_exploded_london.reset_index(drop=True), encoded_df.reset_index(drop=True)], axis=1)

# Reaggregate
empl_geog_quality_places_exploded_london.drop(columns=['category_list'], inplace=True)

# Group by LSOA11CD - sum numeric only
numeric_columns = encoded_df.columns.tolist()
empl_geog_quality_places_encoded_london = empl_geog_quality_places_exploded_london.groupby('LSOA11CD')[numeric_columns].sum().reset_index()

#Save columns of interest for analysis

quality_categories_london = empl_geog_quality_places_encoded_london.columns[1:]

empl_geog_quality_places_encoded_london.head()



# One-Hot Encoding - yet again on Bham

#Explode category lists
empl_geog_quality_places_exploded_bham = empl_geog_quality_places_bham.explode('category_list')

#Set up encoder
encoder = OneHotEncoder(sparse_output=False)

#Encode
encoded_categories_quality = encoder.fit_transform(empl_geog_quality_places_exploded_bham[['category_list']])
encoded_df = pd.DataFrame(encoded_categories_quality, columns=encoder.get_feature_names_out(['category_list']))

empl_geog_quality_places_exploded_bham = pd.concat([empl_geog_quality_places_exploded_bham.reset_index(drop=True), encoded_df.reset_index(drop=True)], axis=1)

# Reaggregate
empl_geog_quality_places_exploded_bham.drop(columns=['category_list'], inplace=True)

# Group by LSOA11CD - sum numeric only
numeric_columns = encoded_df.columns.tolist()
empl_geog_quality_places_encoded_bham = empl_geog_quality_places_exploded_bham.groupby('LSOA11CD')[numeric_columns].sum().reset_index()

#Save columns of interest for analysis

quality_categories_bham = empl_geog_quality_places_encoded_bham.columns[1:]

empl_geog_quality_places_encoded_bham.head()



#Rejoin with all data

all_data_quality_places_london = pd.merge(empl_geog_quality_places_london, empl_geog_quality_places_encoded_london, on='LSOA11CD')
all_data_quality_places_bham = pd.merge(empl_geog_quality_places_bham, empl_geog_quality_places_encoded_bham, on='LSOA11CD')

all_data_quality_places_london.head()








# Automated Condensing of Column Types

# List all columns

all_columns_london = empl_geog_london_encoded.columns.tolist()

# I'll group them by 'suffix' to find commonalities
suffix_groups = {}

# Iterate through columns to identify suffixes
for column in all_columns_london:
    lower_column = column.lower()
    parts = lower_column.split('_')
    if len(parts) > 1:
        suffix = parts[-1]
        if suffix in suffix_groups:
            suffix_groups[suffix].append(column)
        else:
            suffix_groups[suffix] = [column]
    else:
        # Handle columns without suffixes
        suffix_groups['other'] = suffix_groups.get('other', []) + [column]

# Create a list of DataFrames to concatenate
concatenated_dfs_london = []

# Iterate through suffix groups and aggregate columns
for suffix, columns in suffix_groups.items():
    if columns:
        if len(columns) == 1:
            # Preserve col name if only one col with suffix
            new_column_name = columns[0].replace('category_list_', '')
            concatenated_dfs_london.append(empl_geog_london_encoded[columns].rename(columns={columns[0]: new_column_name}))
        else:
            # Aggregate columns with more than one column in the group
            new_column_name = f'all_{suffix}'
            concatenated_dfs_london.append(empl_geog_london_encoded[columns].sum(axis=1).rename(new_column_name))

# Concatenate all DataFrames
condensed_categories_london = pd.concat(concatenated_dfs_london, axis=1)

# Print summary information (groups only)
for suffix, columns in suffix_groups.items():
    num_columns = len(columns)
    if num_columns > 1:
        print(f'Number of {suffix.capitalize()} Categories: {num_columns}')

condensed_categories_london.head()





# Automated Condensing of Column Types

# List all columns

all_columns_bham = empl_geog_bham_encoded.columns.tolist()

# I'll group them by 'suffix' to find commonalities
suffix_groups = {}

# Iterate through columns to identify suffixes
for column in all_columns_bham:
    lower_column = column.lower()
    parts = lower_column.split('_')
    if len(parts) > 1:
        suffix = parts[-1]
        if suffix in suffix_groups:
            suffix_groups[suffix].append(column)
        else:
            suffix_groups[suffix] = [column]
    else:
        # Handle columns without suffixes
        suffix_groups['other'] = suffix_groups.get('other', []) + [column]

# Create a list of DataFrames to concatenate
concatenated_dfs_bham = []

# Iterate through suffix groups and aggregate columns
for suffix, columns in suffix_groups.items():
    if columns:
        if len(columns) == 1:
            # Preserve col name if only one col with suffix
            new_column_name = columns[0].replace('category_list_', '')
            concatenated_dfs_bham.append(empl_geog_bham_encoded[columns].rename(columns={columns[0]: new_column_name}))
        else:
            # Aggregate columns with more than one column in the group
            new_column_name = f'all_{suffix}'
            concatenated_dfs_bham.append(empl_geog_bham_encoded[columns].sum(axis=1).rename(new_column_name))

# Concatenate all DataFrames
condensed_categories_bham = pd.concat(concatenated_dfs_bham, axis=1)

# Print summary information (groups only)
for suffix, columns in suffix_groups.items():
    num_columns = len(columns)
    if num_columns > 1:
        print(f'Number of {suffix.capitalize()} Categories: {num_columns}')

condensed_categories_bham.head()


#Remove all categories that have less than five in London

column_sums_london = condensed_categories_london.iloc[:, 1:].sum()
columns_to_drop_london = column_sums_london[column_sums_london < 5].index.tolist()
print(columns_to_drop_london)

condensed_categories_london = condensed_categories_london.drop(columns=columns_to_drop_london)

#Store new categories
condensed_category_columns_london = condensed_categories_london.columns[1:]

condensed_categories_london.head()


#Remove all categories that have less than five in Birmingham

column_sums_bham = condensed_categories_bham.iloc[:, 1:].sum()
columns_to_drop_bham = column_sums_bham[column_sums_bham < 5].index.tolist()
print(columns_to_drop_bham)

condensed_categories_bham = condensed_categories_bham.drop(columns=columns_to_drop_bham)

#Store new categories
condensed_category_columns_bham = condensed_categories_bham.columns[1:]

condensed_categories_bham.head()


# Join data

all_data_cleaned_london = pd.merge(data_with_geom_london, condensed_categories_london, on='LSOA11CD')
all_data_cleaned_bham = pd.merge(data_with_geom_bham, condensed_categories_bham, on='LSOA11CD')

all_data_cleaned_london.head()





# Create a new column Employment Density and create new model to target that
all_data_cleaned_london['employment_density'] = all_data_cleaned_london['total_employment'] / (all_data_cleaned_london['geometry']).to_crs("EPSG:27700").area
all_data_cleaned_bham['employment_density'] = all_data_cleaned_bham['total_employment'] / (all_data_cleaned_bham['geometry']).to_crs("EPSG:27700").area


# Use office work cols from previous analysis

office_work_cols = [
    '42 : Civil engineering',
    '58 : Publishing activities',
    '59 : Motion picture, video and television programme production, sound recording and music publishing activities',
    '60 : Programming and broadcasting activities',
    '61 : Telecommunications',
    '62 : Computer programming, consultancy and related activities',
    '63 : Information service activities',
    '64 : Financial service activities, except insurance and pension funding',
    '65 : Insurance, reinsurance and pension funding, except compulsory social security',
    '66 : Activities auxiliary to financial services and insurance activities',
    '68 : Real estate activities',
    '69 : Legal and accounting activities',
    '70 : Activities of head offices; management consultancy activities',
    '71 : Architectural and engineering activities; technical testing and analysis',
    '72 : Scientific research and development',
    '73 : Advertising and market research',
    '74 : Other professional, scientific and technical activities',
    '77 : Rental and leasing activities',
    '78 : Employment activities',
    '79 : Travel agency, tour operator and other reservation service and related activities',
    '80 : Security and investigation activities',
    '82 : Office administrative, office support and other business support activities',
    '84 : Public administration and defence; compulsory social security'
]

# Create a new Office Work Total Column :
all_data_cleaned_london['office_total_employment'] = all_data_cleaned_london[office_work_cols].sum(axis=1)
all_data_cleaned_bham['office_total_employment'] = all_data_cleaned_bham[office_work_cols].sum(axis=1)

# And an Office Work Density Column
all_data_cleaned_london['office_employment_density'] = all_data_cleaned_london['office_total_employment'] / (all_data_cleaned_london['geometry']).to_crs("EPSG:27700").area
all_data_cleaned_bham['office_employment_density'] = all_data_cleaned_bham['office_total_employment'] / (all_data_cleaned_bham['geometry']).to_crs("EPSG:27700").area

all_data_cleaned_bham.head()






# Save CSV

all_data_cleaned_london.to_csv("data/combined_data/all_data_london.csv")
all_data_cleaned_bham.to_csv("data/combined_data/all_data_bham.csv")


# Save London to Dataframe on Desktop

# Convert multipolygons to WKT for export
all_data_cleaned_london['building_poly'] = all_data_cleaned_london['building_poly'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)
all_data_cleaned_london['commercial_building_poly'] = all_data_cleaned_london['commercial_building_poly'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)
all_data_cleaned_london['retail_building_poly'] = all_data_cleaned_london['retail_building_poly'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)
all_data_cleaned_london['office_building_poly'] = all_data_cleaned_london['office_building_poly'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)
all_data_cleaned_london['residential_building_poly'] = all_data_cleaned_london['residential_building_poly'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)

all_data_cleaned_london['place_points'] = all_data_cleaned_london['place_points'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)

# Convert category_list to string for export
all_data_cleaned_london['category_list'] = all_data_cleaned_london['category_list'].apply(lambda x: str(x) if x else None)

# Export
all_data_cleaned_london.to_file("data/combined_data/all_data_london.geojson", driver="GeoJSON")


# Save Birmingham to Dataframe on Desktop

# Convert multipolygons to WKT for export
all_data_cleaned_bham['building_poly'] = all_data_cleaned_bham['building_poly'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)
all_data_cleaned_bham['commercial_building_poly'] = all_data_cleaned_bham['commercial_building_poly'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)
all_data_cleaned_bham['retail_building_poly'] = all_data_cleaned_bham['retail_building_poly'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)
all_data_cleaned_bham['office_building_poly'] = all_data_cleaned_bham['office_building_poly'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)
all_data_cleaned_bham['residential_building_poly'] = all_data_cleaned_bham['residential_building_poly'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)

all_data_cleaned_bham['place_points'] = all_data_cleaned_bham['place_points'].apply(lambda x: x.wkt if isinstance(x, shapely.geometry.base.BaseGeometry) else x)

# Convert category_list to string for export
all_data_cleaned_bham['category_list'] = all_data_cleaned_bham['category_list'].apply(lambda x: str(x) if x else None)

# Export
all_data_cleaned_bham.to_file("data/combined_data/all_data_bham.geojson", driver="GeoJSON")





# London
all_data_cleaned_london = gpd.read_file("data/combined_data/all_data_london.geojson")

# WKT strings back to Shapely geometries
all_data_cleaned_london['building_poly'] = all_data_cleaned_london['building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
all_data_cleaned_london['commercial_building_poly'] = all_data_cleaned_london['commercial_building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
all_data_cleaned_london['retail_building_poly'] = all_data_cleaned_london['retail_building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
all_data_cleaned_london['office_building_poly'] = all_data_cleaned_london['office_building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
all_data_cleaned_london['residential_building_poly'] = all_data_cleaned_london['residential_building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)

all_data_cleaned_london['place_points'] = all_data_cleaned_london['place_points'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
# Convert string representations of lists back to lists
all_data_cleaned_london['category_list'] = all_data_cleaned_london['category_list'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)

# Birmingham
all_data_cleaned_bham = gpd.read_file("data/combined_data/all_data_bham.geojson")

# WKT strings back to Shapely geometries
all_data_cleaned_bham['building_poly'] = all_data_cleaned_bham['building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
all_data_cleaned_bham['commercial_building_poly'] = all_data_cleaned_bham['commercial_building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
all_data_cleaned_bham['retail_building_poly'] = all_data_cleaned_bham['retail_building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
all_data_cleaned_bham['office_building_poly'] = all_data_cleaned_bham['office_building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
all_data_cleaned_bham['residential_building_poly'] = all_data_cleaned_bham['residential_building_poly'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)

all_data_cleaned_bham['place_points'] = all_data_cleaned_bham['place_points'].apply(lambda x: shapely.wkt.loads(x) if isinstance(x, str) else x)
# Convert string representations of lists back to lists
all_data_cleaned_bham['category_list'] = all_data_cleaned_bham['category_list'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)

# Test
all_data_cleaned_london.head()





# Construct London Adjacency Matrix (K = 6)

# Calculate centroids (British National Grid)

all_data_cleaned_london['centroid'] = all_data_cleaned_london.geometry.to_crs("EPSG:27700").centroid

# Extract centroid coordinates and construct KDTree
centroids = np.array(list(all_data_cleaned_london['centroid'].apply(lambda geom: (geom.x, geom.y))))
tree = KDTree(centroids)

# Identify six nearest neighbours (+ self point)
distances_london, indices_london = tree.query(centroids, k=7)

# Store results
nearest_neighbors_london = {}
for idx, neighbors in enumerate(indices_london):      
    nearest_neighbors_london[idx] = neighbors[1:]

neighbors_df_london = pd.DataFrame.from_dict(nearest_neighbors_london, orient='index')

# Appply to all data
all_data_cleaned_london['nearest_neighbors'] = all_data_cleaned_london.index.map(nearest_neighbors_london)

all_data_cleaned_london.head()


# Construct Birmingam Adjacency Matrix (K = 6)

# Calculate centroids (British National Grid)

all_data_cleaned_bham['centroid'] = all_data_cleaned_bham.geometry.to_crs("EPSG:27700").centroid

# Extract centroid coordinates and construct KDTree
centroids = np.array(list(all_data_cleaned_bham['centroid'].apply(lambda geom: (geom.x, geom.y))))
tree = KDTree(centroids)

# Identify six nearest neighbours (+ self point)
distances_bham, indices_bham = tree.query(centroids, k=7)

# Store results
nearest_neighbors_bham = {}
for idx, neighbors in enumerate(indices_bham):
    nearest_neighbors_bham[idx] = neighbors[1:]

neighbors_df_bham = pd.DataFrame.from_dict(nearest_neighbors_bham, orient='index')

# Appply to all data
all_data_cleaned_bham['nearest_neighbors'] = all_data_cleaned_bham.index.map(nearest_neighbors_bham)

all_data_cleaned_bham.head()


# Construct adjacent features and add back to feature list

# Fix Population
all_data_cleaned_london['population'] = all_data_cleaned_london['population'].str.replace(',', '').astype(int)

# Get relevant features
feature_columns_london = ['log_num_buildings', 'log_num_places', 'population'] + list(geo_features_london) + list(condensed_category_columns_london)


with open("data/combined_data/feature_columns_london.csv", 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerows(feature_columns_london)
    
# Create new columns
for col in feature_columns_london:
    all_data_cleaned_london[f'lag_{col}'] = None

# Store average of adjacent features
average_features_london = {f'lag_{col}': [] for col in feature_columns_london}

# For each row in the dataframe
for idx, row in all_data_cleaned_london.iterrows():

    #Track progress
    if idx % 20 == 0:
        print(idx)
    # Get the six closest LSOAs computed earlier
    neighbor_indices = row['nearest_neighbors']
    
    # Calculate the average for each feature across those six
    for col in feature_columns_london:
        avg_value = all_data_cleaned_london.iloc[neighbor_indices][col].mean()
        average_features_london[f'lag_{col}'].append(avg_value)

# Add the lagged features to the original GeoDataFrame
for col in average_features_london:
    all_data_cleaned_london[col] = average_features_london[col]

# Display the result
all_data_cleaned_london.head()


# Construct adjacent features and add back to feature list

# Fix Population
all_data_cleaned_bham['population'] = all_data_cleaned_bham['population'].str.replace(',', '').astype(int)

# Get relevant features
feature_columns_bham = ['log_num_buildings', 'log_num_places', 'population'] + list(geo_features_bham) + list(condensed_category_columns_bham)

# Save feature list to csv
with open("data/combined_data/feature_columns_bham.csv", 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerows(feature_columns_bham)

# Create new columns
for col in feature_columns_bham:
    all_data_cleaned_bham[f'lag_{col}'] = None

# Store average of adjacent features
average_features_bham = {f'lag_{col}': [] for col in feature_columns_bham}

# For each row in the dataframe
for idx, row in all_data_cleaned_bham.iterrows():

    #Track progress
    if idx % 20 == 0:
        print(idx)
    # Get the six closest LSOAs computed earlier
    neighbor_indices = row['nearest_neighbors']
    
    # Calculate the average for each feature across those six
    for col in feature_columns_bham:
        avg_value = all_data_cleaned_bham.iloc[neighbor_indices][col].mean()
        average_features_bham[f'lag_{col}'].append(avg_value)

# Add the lagged features to the original GeoDataFrame
for col in average_features_bham:
    all_data_cleaned_bham[col] = average_features_bham[col]

# Display the result
all_data_cleaned_bham.head()


# Set up total feature lists

lag_feature_columns_london = [f'lag_{col}' for col in feature_columns_london]
total_feature_columns_london = feature_columns_london + lag_feature_columns_london

lag_feature_columns_bham = [f'lag_{col}' for col in feature_columns_bham]
total_feature_columns_bham = feature_columns_bham + lag_feature_columns_bham

# Save feature lists to csvs
with open("data/combined_data/total_feature_columns_london.csv", 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerows(total_feature_columns_london)

with open("data/combined_data/total_feature_columns_bham.csv", 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerows(total_feature_columns_bham)


# Save CSVs

all_data_cleaned_london.to_csv("data/combined_data/lag/all_data_london_lag.csv")
all_data_cleaned_bham.to_csv("data/combined_data/lag/all_data_bham_lag.csv")
